{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_paths  = ['/kaggle/input/people-detection/dataset1/df.csv', '/kaggle/input/people-detection/dataset2/df.csv', '/kaggle/input/people-detection/dataset3/df.csv']  # Add more file names as needed\n",
    "dfs = []\n",
    "\n",
    "for csv_path in csv_paths:\n",
    "    df = pd.read_csv(csv_path, header=None, names=['image_path', 'mask_path', 'collage_path'])\n",
    "\n",
    "    base_folder = os.path.dirname(csv_path)\n",
    "\n",
    "    df['image_path'] = df['image_path'].apply(lambda x: os.path.join(base_folder, x))\n",
    "    df['mask_path'] = df['mask_path'].apply(lambda x: os.path.join(base_folder, x))\n",
    "    df['collage_path'] = df['collage_path'].apply(lambda x: os.path.join(base_folder, x))\n",
    "\n",
    "    df = df[df['image_path'].str.endswith('.png') & df['mask_path'].str.endswith('.png') & df['collage_path'].str.endswith('.jpg')]\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "concatenated_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating my own data generator because the data is too large to fit in memory at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataframe, batch_size, img_size):\n",
    "        self.dataframe = dataframe\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_df = self.dataframe.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        X = np.zeros((self.batch_size, *self.img_size, 3), dtype=np.float32)\n",
    "        Y = np.zeros((self.batch_size, *self.img_size, 1), dtype=np.float32)\n",
    "        \n",
    "        for i, (_, row) in enumerate(batch_df.iterrows()):\n",
    "            img = Image.open(row['image_path']).convert('RGB').resize(self.img_size)\n",
    "            mask = Image.open(row['mask_path']).convert('L').resize(self.img_size)\n",
    "            \n",
    "            X[i] = np.array(img) / 255.0\n",
    "            Y[i] = np.expand_dims(np.array(mask), axis=-1) / 255.0\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def unet_model(input_size=(256, 256, 3)):\n",
    "    inputs = layers.Input(input_size)\n",
    "    \n",
    "    # Encoder (Downsampling)\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.BatchNormalization()(conv1)\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    conv1 = layers.BatchNormalization()(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = layers.Dropout(0.5)(pool1)\n",
    "    \n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.BatchNormalization()(conv2)\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    conv2 = layers.BatchNormalization()(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = layers.Dropout(0.5)(pool2)\n",
    "    \n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = layers.BatchNormalization()(conv3)\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    conv3 = layers.BatchNormalization()(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = layers.Dropout(0.5)(pool3)\n",
    "    \n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = layers.BatchNormalization()(conv4)\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    conv4 = layers.BatchNormalization()(conv4)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    pool4 = layers.Dropout(0.5)(pool4)\n",
    "    \n",
    "    # Bridge\n",
    "    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = layers.BatchNormalization()(conv5)\n",
    "    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    conv5 = layers.BatchNormalization()(conv5)\n",
    "    \n",
    "    # Decoder (Upsampling)\n",
    "    up6 = layers.Conv2D(512, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv5))\n",
    "    up6 = layers.BatchNormalization()(up6)\n",
    "    merge6 = layers.concatenate([conv4, up6], axis=3)\n",
    "    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "    conv6 = layers.BatchNormalization()(conv6)\n",
    "    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "    conv6 = layers.BatchNormalization()(conv6)\n",
    "    \n",
    "    up7 = layers.Conv2D(256, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv6))\n",
    "    up7 = layers.BatchNormalization()(up7)\n",
    "    merge7 = layers.concatenate([conv3, up7], axis=3)\n",
    "    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "    conv7 = layers.BatchNormalization()(conv7)\n",
    "    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "    conv7 = layers.BatchNormalization()(conv7)\n",
    "    \n",
    "    up8 = layers.Conv2D(128, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv7))\n",
    "    up8 = layers.BatchNormalization()(up8)\n",
    "    merge8 = layers.concatenate([conv2, up8], axis=3)\n",
    "    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "    conv8 = layers.BatchNormalization()(conv8)\n",
    "    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "    conv8 = layers.BatchNormalization()(conv8)\n",
    "    \n",
    "    up9 = layers.Conv2D(64, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv8))\n",
    "    up9 = layers.BatchNormalization()(up9)\n",
    "    merge9 = layers.concatenate([conv1, up9], axis=3)\n",
    "    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "    conv9 = layers.BatchNormalization()(conv9)\n",
    "    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "    conv9 = layers.BatchNormalization()(conv9)\n",
    "    \n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the model and creating the training and validation data generators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "train_df = concatenated_df.sample(frac=0.8, random_state=42)\n",
    "val_df = concatenated_df.drop(train_df.index)\n",
    "\n",
    "train_generator = DataGenerator(train_df, 16, (256, 256))\n",
    "val_generator = DataGenerator(val_df, 16, (256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#publishing generator\n",
    "train_generator = DataGenerator(concatenated_df, 16, (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    #validation_data=val_generator,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=3)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model and the weights to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('remove_background_weights.h5')\n",
    "model.save('remove_background_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some model testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('remove_background_model.h5', compile=False)\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path, img_size):\n",
    "    img = Image.open(image_path).convert('RGB').resize(img_size)\n",
    "    img_array = np.array(img) / 255.0\n",
    "    return np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Function to postprocess the mask\n",
    "def postprocess_mask(mask):\n",
    "    mask = (mask > 0.5).astype(np.uint8)\n",
    "    return mask.squeeze()\n",
    "\n",
    "# Function to apply mask to the image\n",
    "def apply_mask(image, mask):\n",
    "    return image * np.expand_dims(mask, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test image path\n",
    "test_image_path = '/kaggle/input/people-detection/dataset3/images/ds11_bluebells-woods-english-spring-160972.png'\n",
    "\n",
    "# Derive expected mask path from the image path\n",
    "expected_mask_path = os.path.join('/kaggle/input/people-detection/dataset3/masks', os.path.basename(test_image_path))\n",
    "\n",
    "# Preprocess the image\n",
    "input_image = preprocess_image(test_image_path, (256, 256))\n",
    "\n",
    "# Predict the mask\n",
    "predicted_mask = model.predict(input_image)\n",
    "predicted_mask = postprocess_mask(predicted_mask)\n",
    "\n",
    "# Load the original image for display\n",
    "original_image = Image.open(test_image_path).resize((256, 256))\n",
    "original_image_np = np.array(original_image) / 255.0\n",
    "\n",
    "# Load the expected mask for display\n",
    "expected_mask = Image.open(expected_mask_path).convert('L').resize((256, 256))\n",
    "expected_mask_np = np.array(expected_mask) / 255.0\n",
    "\n",
    "# Apply the mask to the original image\n",
    "cropped_image = apply_mask(original_image_np, predicted_mask)\n",
    "\n",
    "# Create an RGBA image with the mask as the alpha channel\n",
    "rgba_image = np.zeros((256, 256, 4), dtype=np.float32)\n",
    "rgba_image[..., :3] = cropped_image\n",
    "rgba_image[..., 3] = predicted_mask\n",
    "\n",
    "# Convert the RGBA image to PIL Image format\n",
    "rgba_image_pil = Image.fromarray((rgba_image * 255).astype(np.uint8), 'RGBA')\n",
    "\n",
    "# Save the RGBA image with transparent background\n",
    "rgba_image_pil.save('cropped_image_with_transparent_background.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting of the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original image, predicted mask, expected mask, and cropped image\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(original_image_np)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title('Expected Mask')\n",
    "plt.imshow(expected_mask_np, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('Predicted Mask')\n",
    "plt.imshow(predicted_mask, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title('Cropped Image with Transparent Background')\n",
    "plt.imshow(rgba_image_pil)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
